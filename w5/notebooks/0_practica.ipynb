{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066c805c-59f3-497a-8691-b8e80052e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-07 15:34:32--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz\n",
      "20.201.28.151, 205.251.194.8, 205.251.193.165, ...\n",
      "connected. to github.com (github.com)|20.201.28.151|:443... \n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/d3904232-1a2b-431b-803d-0ee802cd14fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250307%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250307T183432Z&X-Amz-Expires=300&X-Amz-Signature=7d53354f786712cf5802ce890e8407c717f2db286026364ded6ad706fd4a21fb&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-03-07 15:34:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/d3904232-1a2b-431b-803d-0ee802cd14fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250307%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250307T183432Z&X-Amz-Expires=300&X-Amz-Signature=7d53354f786712cf5802ce890e8407c717f2db286026364ded6ad706fd4a21fb&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "connected. to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... \n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 11099245 (11M) [application/octet-stream]\n",
      "Saving to: ‘green_tripdata_2019-01.csv.gz’\n",
      "\n",
      "green_tripdata_2019 100%[===================>]  10.58M  9.34MB/s    in 1.1s    \n",
      "\n",
      "2025-03-07 15:34:34 (9.34 MB/s) - ‘green_tripdata_2019-01.csv.gz’ saved [11099245/11099245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5cbbed3-ca7c-467a-874d-db6a4d8b57bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homework5.ipynb  green_tripdata_2019-01.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "874d80f5-cd22-4c6e-b4ae-db7932d1125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip green_tripdata_2019-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29cd747-ccb7-4b54-bf24-393d103a392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 13:29:11 WARN Utils: Your hostname, LAPTOP-F5VFQF4J resolves to a loopback address: 127.0.1.1; using 172.26.218.55 instead (on interface eth0)\n",
      "25/03/10 13:29:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 13:29:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Ejemplos\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a02a2fb-64b4-42f2-aefc-96b27721d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_taxis = spark.read.csv(\"green_tripdata_2019-01.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b5118a-9af7-4790-b48e-3ee8ed20d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm green_tripdata_2019-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72675d30-d84f-45f5-a9f3-898e11e01c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxis.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49991c08-49b7-4b50-afb1-87836a40481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, when, year, month, dayofweek, unix_timestamp\n",
    "\n",
    "# Convertimos las columnas de fecha y hora a timestamps\n",
    "df_taxis = df_taxis.withColumn(\"pickup_datetime_ts\", to_timestamp(col(\"lpep_pickup_datetime\"))) \\\n",
    "                   .withColumn(\"dropoff_datetime_ts\", to_timestamp(col(\"lpep_dropoff_datetime\")))\n",
    "\n",
    "# Creamos una columna con la duración del viaje en minutos\n",
    "df_taxis = df_taxis.withColumn(\"duracion_viaje_minutos\", \n",
    "                             (unix_timestamp(col(\"dropoff_datetime_ts\")) - unix_timestamp(col(\"pickup_datetime_ts\"))) / 60)\n",
    "\n",
    "# Añadimos columnas con el año, mes y día de la semana del viaje\n",
    "df_taxis = df_taxis.withColumn(\"anho\", year(col(\"pickup_datetime_ts\"))) \\\n",
    "                   .withColumn(\"mes\", month(col(\"pickup_datetime_ts\"))) \\\n",
    "                   .withColumn(\"dia_semana\", dayofweek(col(\"pickup_datetime_ts\")))\n",
    "\n",
    "# Filtramos los viajes que duraron más de una hora\n",
    "df_taxis_largos = df_taxis.filter(col(\"duracion_viaje_minutos\") > 60)\n",
    "\n",
    "# Creamos una columna que clasifica los viajes según la hora del día\n",
    "df_taxis = df_taxis.withColumn(\"hora_dia\", \n",
    "                             when((col(\"pickup_datetime_ts\").between(\"06:00:00\", \"11:59:59\")), \"mañana\")\n",
    "                             .when((col(\"pickup_datetime_ts\").between(\"12:00:00\", \"17:59:59\")), \"tarde\")\n",
    "                             .when((col(\"pickup_datetime_ts\").between(\"18:00:00\", \"23:59:59\")), \"noche\")\n",
    "                             .otherwise(\"madrugada\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66e58dc4-b520-45e0-89d9-7a3fc6a8e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------------------+----+---+----------+---------+\n",
      "| pickup_datetime_ts|dropoff_datetime_ts|duracion_viaje_minutos|anho|mes|dia_semana| hora_dia|\n",
      "+-------------------+-------------------+----------------------+----+---+----------+---------+\n",
      "|2018-12-21 15:17:29|2018-12-21 15:18:57|    1.4666666666666666|2018| 12|         6|madrugada|\n",
      "|2019-01-01 00:10:16|2019-01-01 00:16:32|     6.266666666666667|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:27:11|2019-01-01 00:31:38|                  4.45|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:46:20|2019-01-01 01:04:54|    18.566666666666666|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:19:06|2019-01-01 00:39:43|    20.616666666666667|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:12:35|2019-01-01 00:19:09|     6.566666666666666|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:47:55|2019-01-01 01:00:01|                  12.1|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:12:47|2019-01-01 00:30:50|                 18.05|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:16:23|2019-01-01 00:39:46|    23.383333333333333|2019|  1|         3|madrugada|\n",
      "|2019-01-01 00:58:02|2019-01-01 01:19:02|                  21.0|2019|  1|         3|madrugada|\n",
      "+-------------------+-------------------+----------------------+----+---+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxis \\\n",
    ".select(col(\"pickup_datetime_ts\"), col(\"dropoff_datetime_ts\"), col(\"duracion_viaje_minutos\"), col(\"anho\"), col(\"mes\"), col(\"dia_semana\"), col(\"hora_dia\")) \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c54bfdc0-7438-4f8b-b00f-6bbb0051fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Definimos una función en Python para convertir millas a kilómetros\n",
    "def millas_a_kilometros(millas):\n",
    "  return millas * 1.60934\n",
    "\n",
    "# Registramos la función como una UDF de Spark\n",
    "millas_a_kilometros_udf = udf(millas_a_kilometros, returnType=FloatType())\n",
    "\n",
    "# Aplicamos la UDF a nuestro DataFrame\n",
    "df_taxis = df_taxis.withColumn(\"distancia_km\", millas_a_kilometros_udf(col(\"trip_distance\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "609aea26-5d23-4024-917b-5c72ee0671de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+-------------+\n",
      "| pickup_datetime_ts|dropoff_datetime_ts|distancia_km|trip_distance|\n",
      "+-------------------+-------------------+------------+-------------+\n",
      "|2018-12-21 15:17:29|2018-12-21 15:18:57|         0.0|          0.0|\n",
      "|2019-01-01 00:10:16|2019-01-01 00:16:32|   1.3840324|         0.86|\n",
      "|2019-01-01 00:27:11|2019-01-01 00:31:38|   1.0621644|         0.66|\n",
      "|2019-01-01 00:46:20|2019-01-01 01:04:54|    4.313031|         2.68|\n",
      "|2019-01-01 00:19:06|2019-01-01 00:39:43|   7.2903104|         4.53|\n",
      "|2019-01-01 00:12:35|2019-01-01 00:19:09|    1.689807|         1.05|\n",
      "|2019-01-01 00:47:55|2019-01-01 01:00:01|   6.0672116|         3.77|\n",
      "|2019-01-01 00:12:47|2019-01-01 00:30:50|    6.598294|          4.1|\n",
      "|2019-01-01 00:16:23|2019-01-01 00:39:46|   12.472385|         7.75|\n",
      "|2019-01-01 00:58:02|2019-01-01 01:19:02|   5.9223714|         3.68|\n",
      "+-------------------+-------------------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxis \\\n",
    ".select(col(\"pickup_datetime_ts\"), col(\"dropoff_datetime_ts\"), col(\"distancia_km\"), col(\"trip_distance\")) \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baf0015a-b897-4258-bfcf-34c57136bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+------------------+\n",
      "| pickup_datetime_ts|dropoff_datetime_ts|trip_distance|      distancia_km|\n",
      "+-------------------+-------------------+-------------+------------------+\n",
      "|2018-12-21 15:17:29|2018-12-21 15:18:57|          0.0|               0.0|\n",
      "|2019-01-01 00:10:16|2019-01-01 00:16:32|         0.86|         1.3840324|\n",
      "|2019-01-01 00:27:11|2019-01-01 00:31:38|         0.66|1.0621644000000001|\n",
      "|2019-01-01 00:46:20|2019-01-01 01:04:54|         2.68|         4.3130312|\n",
      "|2019-01-01 00:19:06|2019-01-01 00:39:43|         4.53|         7.2903102|\n",
      "|2019-01-01 00:12:35|2019-01-01 00:19:09|         1.05|          1.689807|\n",
      "|2019-01-01 00:47:55|2019-01-01 01:00:01|         3.77|         6.0672118|\n",
      "|2019-01-01 00:12:47|2019-01-01 00:30:50|          4.1| 6.598293999999999|\n",
      "|2019-01-01 00:16:23|2019-01-01 00:39:46|         7.75|         12.472385|\n",
      "|2019-01-01 00:58:02|2019-01-01 01:19:02|         3.68| 5.922371200000001|\n",
      "+-------------------+-------------------+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registramos el DataFrame como una tabla temporal\n",
    "df_taxis.createOrReplaceTempView(\"taxis\")\n",
    "\n",
    "# Usamos SQL para crear una nueva columna con la distancia en kilómetros\n",
    "df_taxis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime_ts, \n",
    "    dropoff_datetime_ts, \n",
    "    trip_distance, \n",
    "    trip_distance * 1.60934 AS distancia_km \n",
    "FROM taxis\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a903c4-824d-41df-9081-30efde158b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------+\n",
      "|id_viaje|zona_id|zona_id|nombre_barrio|\n",
      "+--------+-------+-------+-------------+\n",
      "|       1|      1|      1|    Manhattan|\n",
      "|       3|      1|      1|    Manhattan|\n",
      "|       2|      2|      2|     Brooklyn|\n",
      "+--------+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "viajes_df = spark.createDataFrame(\n",
    "    [(1, 1), (2, 2), (3, 1)], [\"id_viaje\", \"zona_id\"]\n",
    ")\n",
    "zonas_df = spark.createDataFrame(\n",
    "    [(1, \"Manhattan\"), (2, \"Brooklyn\")], [\"zona_id\", \"nombre_barrio\"]\n",
    ")\n",
    "\n",
    "viajes_con_barrio_df = viajes_df.join(\n",
    "    zonas_df, viajes_df.zona_id == zonas_df.zona_id, \"inner\"\n",
    ")\n",
    "\n",
    "viajes_con_barrio_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf45c9b-6bf0-4aac-854d-8f3d6fded424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|zona_id|conteo_viajes|\n",
      "+-------+-------------+\n",
      "|      1|            3|\n",
      "|      2|            1|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EjemploGroupBy\").getOrCreate()\n",
    "\n",
    "viajes_df = spark.createDataFrame(\n",
    "    [(1, 1), (2, 2), (3, 1), (4, 1)], [\"id_viaje\", \"zona_id\"]\n",
    ")\n",
    "\n",
    "viajes_por_zona_df = viajes_df.groupBy(\"zona_id\").agg(count(\"*\").alias(\"conteo_viajes\"))\n",
    "\n",
    "viajes_por_zona_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2976e97-2de6-46f0-8481-5488d3299da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|id_viaje|nombre_barrio|\n",
      "+--------+-------------+\n",
      "|       1|    Manhattan|\n",
      "|       3|    Manhattan|\n",
      "|       2|     Brooklyn|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "viajes_df = spark.createDataFrame(\n",
    "    [(1, 1), (2, 2), (3, 1)], [\"id_viaje\", \"zona_id\"]\n",
    ")\n",
    "zonas_df = spark.createDataFrame(\n",
    "    [(1, \"Manhattan\"), (2, \"Brooklyn\")], [\"zona_id\", \"nombre_barrio\"]\n",
    ")\n",
    "\n",
    "viajes_df.createOrReplaceTempView(\"viajes\")\n",
    "zonas_df.createOrReplaceTempView(\"zonas\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT v.id_viaje, z.nombre_barrio\n",
    "    FROM viajes v\n",
    "    JOIN zonas z ON v.zona_id = z.zona_id\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7a9108-7821-4060-afc4-41bd642447cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|nombre_barrio|distancia_promedio|\n",
      "+-------------+------------------+\n",
      "|       Queens|               7.1|\n",
      "|     Brooklyn|               9.7|\n",
      "|    Manhattan|               4.5|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Datos de ejemplo\n",
    "viajes_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 1, 5.2), \n",
    "        (2, 2, 10.5), \n",
    "        (3, 1, 3.8), \n",
    "        (4, 3, 7.1), \n",
    "        (5, 2, 8.9)\n",
    "    ], \n",
    "    [\"id_viaje\", \"zona_id\", \"distancia\"]\n",
    ")\n",
    "zonas_df = spark.createDataFrame(\n",
    "    [(1, \"Manhattan\"), (2, \"Brooklyn\"), (3, \"Queens\")], \n",
    "    [\"zona_id\", \"nombre_barrio\"]\n",
    ")\n",
    "\n",
    "# Calcular la distancia promedio por zona\n",
    "distancia_promedio_df = (\n",
    "    viajes_df\n",
    "    .join(zonas_df, on=\"zona_id\", how=\"inner\")\n",
    "    .groupBy(\"nombre_barrio\")\n",
    "    .agg(avg(\"distancia\").alias(\"distancia_promedio\"))\n",
    ")\n",
    "\n",
    "distancia_promedio_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078bd85d-e0a9-4173-9dcb-3b01a8a3e9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===========================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|nombre_barrio|num_viajes|\n",
      "+-------------+----------+\n",
      "|     Brooklyn|         2|\n",
      "|    Manhattan|         2|\n",
      "|       Queens|         1|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Datos de ejemplo\n",
    "viajes_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 1, 5.2), \n",
    "        (2, 2, 10.5), \n",
    "        (3, 1, 3.8), \n",
    "        (4, 3, 7.1), \n",
    "        (5, 2, 8.9)\n",
    "    ], \n",
    "    [\"id_viaje\", \"zona_id\", \"distancia\"]\n",
    ")\n",
    "zonas_df = spark.createDataFrame(\n",
    "    [(1, \"Manhattan\"), (2, \"Brooklyn\"), (3, \"Queens\")], \n",
    "    [\"zona_id\", \"nombre_barrio\"]\n",
    ")\n",
    "\n",
    "# Registrar los DataFrames como tablas temporales\n",
    "viajes_df.createOrReplaceTempView(\"viajes\")\n",
    "zonas_df.createOrReplaceTempView(\"zonas\")\n",
    "\n",
    "# Consulta SQL para encontrar las zonas con mayor número de viajes\n",
    "zonas_con_mas_viajes_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT z.nombre_barrio, COUNT(v.id_viaje) AS num_viajes\n",
    "    FROM viajes v\n",
    "    JOIN zonas z ON v.zona_id = z.zona_id\n",
    "    GROUP BY z.nombre_barrio\n",
    "    ORDER BY num_viajes DESC\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "zonas_con_mas_viajes_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
